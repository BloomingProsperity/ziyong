"""
AI åä½œæ¥å£ (Brain Interface) - æ™ºèƒ½å‡çº§ç‰ˆ

è¿™ä¸ªæ¨¡å—æ˜¯ Claude AI æ§åˆ¶ Agent çš„ç»Ÿä¸€å…¥å£ã€‚
Claude å¯ä»¥é€šè¿‡è¿™ä¸ªæ¥å£ï¼š
1. æ”¶é›†ç›®æ ‡ç½‘ç«™ä¿¡æ¯
2. æ™ºèƒ½åˆ†æç½‘ç«™ç‰¹å¾å’Œåçˆ¬ç­–ç•¥
3. æ‰§è¡Œäº¤äº’æ“ä½œ
4. è°ƒç”¨ API å¹¶è‡ªåŠ¨é‡è¯•
5. æ‰§è¡Œæ•°æ®æŠ“å–

æ ¸å¿ƒåŠŸèƒ½ï¼š
- æ™ºèƒ½åˆ†æï¼šè‡ªåŠ¨è¯†åˆ«ç½‘ç«™ç±»å‹ã€åçˆ¬ç­‰çº§ã€åŠ å¯†å‚æ•°
- é¢„è®¾é…ç½®ï¼šå†…ç½® 10+ ä¸»æµç½‘ç«™çš„é…ç½®
- æ™ºèƒ½é‡è¯•ï¼šè‡ªåŠ¨å¤„ç†å¤±è´¥ï¼ŒæŒ‡æ•°é€€é¿
- AI å‹å¥½ï¼šæ‰€æœ‰è¾“å‡ºéƒ½ä¼˜åŒ–ä¸º AI æ˜“è¯»æ ¼å¼

ä½¿ç”¨æ–¹å¼ï¼ˆåœ¨ IDE ä¸­ç”± Claude è°ƒç”¨ï¼‰ï¼š

    from unified_agent import Brain

    brain = Brain()

    # æ™ºèƒ½è°ƒæŸ¥ï¼ˆè‡ªåŠ¨åˆ†æï¼‰
    analysis = brain.smart_investigate("https://jd.com/item/123")
    print(analysis.to_ai_report())  # AI å‹å¥½çš„å®Œæ•´æŠ¥å‘Š

    # åŸºäºåˆ†æç»“æœå†³ç­–
    if analysis.recommended_approach == "direct_api":
        result = brain.call_api(...)
    else:
        result = brain.scrape_page(...)
"""

from __future__ import annotations

import json
import logging
import time
import random
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Literal, Callable

import httpx

from ..core.config import AgentConfig
from ..scraper.collector import InfoCollector, SiteReport
from ..scraper.agent import ScraperAgent, ScrapeResult
from ..infra.proxy import ProxyManager
from ..scraper.smart_analyzer import SmartAnalyzer, SiteAnalysis, SignatureAnalysis
from ..scraper.presets import get_preset, get_preset_info, list_presets, SitePreset
from ..infra.cookie_pool import CookiePool, CookiePoolManager, InfrastructureAdvisor, CookieSession

logger = logging.getLogger(__name__)


@dataclass
class ApiCallResult:
    """API è°ƒç”¨ç»“æœ"""
    success: bool
    status_code: int | None
    headers: dict[str, str]
    body: str | dict | list | None
    error: str | None = None
    retries: int = 0
    response_time_ms: float = 0

    def to_ai_summary(self) -> str:
        """ç”Ÿæˆ AI å‹å¥½çš„æ‘˜è¦"""
        if self.success:
            body_preview = ""
            if isinstance(self.body, dict):
                body_preview = f"è¿”å› JSON å¯¹è±¡ï¼ŒåŒ…å« {len(self.body)} ä¸ªå­—æ®µ"
            elif isinstance(self.body, list):
                body_preview = f"è¿”å› JSON æ•°ç»„ï¼ŒåŒ…å« {len(self.body)} æ¡æ•°æ®"
            elif isinstance(self.body, str):
                body_preview = f"è¿”å›æ–‡æœ¬ï¼Œé•¿åº¦ {len(self.body)} å­—ç¬¦"

            return f"[OK] è¯·æ±‚æˆåŠŸ | çŠ¶æ€ç : {self.status_code} | {body_preview} | è€—æ—¶: {self.response_time_ms:.0f}ms"
        else:
            return f"[FAIL] è¯·æ±‚å¤±è´¥ | çŠ¶æ€ç : {self.status_code} | é”™è¯¯: {self.error} | é‡è¯•æ¬¡æ•°: {self.retries}"


@dataclass
class RetryConfig:
    """é‡è¯•é…ç½®"""
    max_retries: int = 3
    base_delay: float = 1.0  # åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰
    max_delay: float = 30.0  # æœ€å¤§å»¶è¿Ÿ
    exponential_base: float = 2.0  # æŒ‡æ•°åŸºæ•°
    jitter: bool = True  # æ˜¯å¦æ·»åŠ éšæœºæŠ–åŠ¨
    retry_on_status: tuple[int, ...] = (429, 500, 502, 503, 504)  # è¿™äº›çŠ¶æ€ç ä¼šé‡è¯•


class Brain:
    """
    AI å¤§è„‘æ¥å£ - æ™ºèƒ½å‡çº§ç‰ˆ

    è¿™æ˜¯ Claude æ§åˆ¶ Agent çš„ç»Ÿä¸€å…¥å£ã€‚
    æ‰€æœ‰æ–¹æ³•éƒ½è®¾è®¡ä¸ºæ˜“äº AI è°ƒç”¨å’Œç†è§£ã€‚

    æ ¸å¿ƒèƒ½åŠ›ï¼š
    1. æ™ºèƒ½è°ƒæŸ¥ï¼šè‡ªåŠ¨è¯†åˆ«ç½‘ç«™ç‰¹å¾ã€åçˆ¬ç­–ç•¥
    2. é¢„è®¾é…ç½®ï¼šå†…ç½®ä¸»æµç½‘ç«™é…ç½®
    3. æ™ºèƒ½é‡è¯•ï¼šè‡ªåŠ¨å¤„ç†å¤±è´¥
    4. AI å‹å¥½ï¼šè¾“å‡ºä¼˜åŒ–ä¸º AI æ˜“è¯»æ ¼å¼
    """

    def __init__(self, config: AgentConfig | None = None, retry_config: RetryConfig | None = None):
        """
        åˆå§‹åŒ– Brain

        Args:
            config: Agent é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ä¼ åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            retry_config: é‡è¯•é…ç½®å¯¹è±¡
        """
        self.config = config or AgentConfig()
        self.retry_config = retry_config or RetryConfig()
        self.collector = InfoCollector(self.config)
        self.scraper = ScraperAgent(self.config)
        self.proxy_manager = ProxyManager(self.config)
        self.analyzer = SmartAnalyzer()

        # Cookieæ± å’ŒåŸºç¡€è®¾æ–½è¯„ä¼°
        self.cookie_pool_manager = CookiePoolManager(str(self.config.data_dir / "cookies"))
        self.infra_advisor = InfrastructureAdvisor(self.cookie_pool_manager)

        # å­˜å‚¨æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œä¾›åç»­ä½¿ç”¨
        self._last_report: SiteReport | None = None
        self._last_analysis: SiteAnalysis | None = None
        self._collected_cookies: dict[str, str] = {}
        self._collected_headers: dict[str, str] = {}
        self._request_count: int = 0
        self._error_count: int = 0
        self._current_cookie_session: CookieSession | None = None

    # ==================== ç¬¬ä¸€é˜¶æ®µï¼šä¿¡æ¯æ”¶é›† ====================

    def investigate(
        self,
        url: str,
        wait_seconds: float = 5.0,
        scroll: bool = True,
        take_screenshot: bool = True,
    ) -> SiteReport:
        """
        è°ƒæŸ¥ç›®æ ‡ç½‘ç«™

        è¿™æ˜¯ç¬¬ä¸€æ­¥ï¼Œæ”¶é›†ç½‘ç«™çš„æ‰€æœ‰ä¿¡æ¯ä¾› AI åˆ†æã€‚

        Args:
            url: ç›®æ ‡ URL
            wait_seconds: ç­‰å¾…æ—¶é—´ï¼ˆè®© AJAX åŠ è½½å®Œæˆï¼‰
            scroll: æ˜¯å¦æ»šåŠ¨é¡µé¢
            take_screenshot: æ˜¯å¦æˆªå›¾

        Returns:
            SiteReport å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰æ”¶é›†åˆ°çš„ä¿¡æ¯

        ç¤ºä¾‹ï¼š
            report = brain.investigate("https://jd.com")
            print(report.summary_for_ai())  # æ‰“å°æ‘˜è¦ä¾› AI åˆ†æ
        """
        logger.info(f"Investigating: {url}")
        report = self.collector.collect(
            url=url,
            wait_seconds=wait_seconds,
            scroll=scroll,
            take_screenshot=take_screenshot,
        )

        # å­˜å‚¨ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
        self._last_report = report
        self._update_collected_data(report)

        logger.info(f"Investigation complete. Found {len(report.api_requests)} API requests")
        return report

    def smart_investigate(
        self,
        url: str,
        wait_seconds: float = 5.0,
        scroll: bool = True,
        take_screenshot: bool = True,
    ) -> SiteAnalysis:
        """
        æ™ºèƒ½è°ƒæŸ¥ç›®æ ‡ç½‘ç«™ï¼ˆæ¨èä½¿ç”¨ï¼‰

        è¿™æ˜¯å‡çº§ç‰ˆçš„ investigate æ–¹æ³•ï¼Œä¼šè‡ªåŠ¨ï¼š
        1. æ”¶é›†ç½‘ç«™ä¿¡æ¯
        2. è¯†åˆ«ç½‘ç«™ç±»å‹
        3. è¯„ä¼°åçˆ¬å¼ºåº¦
        4. åˆ†æåŠ å¯†å‚æ•°
        5. ç”Ÿæˆæ“ä½œå»ºè®®

        Args:
            url: ç›®æ ‡ URL
            wait_seconds: ç­‰å¾…æ—¶é—´
            scroll: æ˜¯å¦æ»šåŠ¨é¡µé¢
            take_screenshot: æ˜¯å¦æˆªå›¾

        Returns:
            SiteAnalysis å¯¹è±¡ï¼ŒåŒ…å«æ™ºèƒ½åˆ†æç»“æœ
            ä½¿ç”¨ .to_ai_report() è·å– AI å‹å¥½çš„æŠ¥å‘Š

        ç¤ºä¾‹ï¼š
            analysis = brain.smart_investigate("https://jd.com")
            print(analysis.to_ai_report())  # å®Œæ•´çš„ AI åˆ†ææŠ¥å‘Š
            print(f"åçˆ¬ç­‰çº§: {analysis.anti_scrape_level}")
            print(f"æ¨èæ–¹æ¡ˆ: {analysis.recommended_approach}")
        """
        logger.info(f"Smart investigating: {url}")

        # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰é¢„è®¾é…ç½®
        preset_info = get_preset_info(url)
        if preset_info.get("found"):
            logger.info(f"Found preset for: {preset_info.get('name')}")

        # æ”¶é›†ä¿¡æ¯
        report = self.investigate(url, wait_seconds, scroll, take_screenshot)

        # æ™ºèƒ½åˆ†æ
        analysis = self.analyzer.analyze(report)
        self._last_analysis = analysis

        logger.info(
            f"Smart analysis complete. "
            f"Type: {analysis.site_type}, "
            f"Anti-scrape: {analysis.anti_scrape_level}, "
            f"Approach: {analysis.recommended_approach}"
        )

        return analysis

    def get_recommendation(self, url: str | None = None) -> str:
        """
        è·å–å¿«é€Ÿå»ºè®®

        å¦‚æœå·²ç»åšè¿‡åˆ†æï¼Œç›´æ¥è¿”å›å»ºè®®ï¼›
        å¦åˆ™å…ˆè¿›è¡Œå¿«é€Ÿåˆ†æã€‚

        Returns:
            AI å‹å¥½çš„å»ºè®®æ–‡æœ¬
        """
        if self._last_analysis:
            return self._last_analysis.to_ai_report()

        if url:
            analysis = self.smart_investigate(url)
            return analysis.to_ai_report()

        return "è¯·å…ˆè°ƒç”¨ smart_investigate(url) è¿›è¡Œåˆ†æ"

    def get_preset(self, url: str) -> SitePreset | None:
        """
        è·å–ç½‘ç«™é¢„è®¾é…ç½®

        Args:
            url: URL æˆ–åŸŸå

        Returns:
            SitePreset æˆ– None
        """
        return get_preset(url)

    def list_supported_sites(self) -> list[str]:
        """åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„é¢„è®¾ç½‘ç«™"""
        return list_presets()

    def interact(
        self,
        url: str,
        actions: list[dict[str, Any]],
        wait_between: float = 1.0,
    ) -> SiteReport:
        """
        å¸¦äº¤äº’çš„ä¿¡æ¯æ”¶é›†

        å½“éœ€è¦ç‚¹å‡»æŒ‰é’®ã€å¡«å†™è¡¨å•ç­‰æ“ä½œæ¥è§¦å‘ API è¯·æ±‚æ—¶ä½¿ç”¨ã€‚

        Args:
            url: ç›®æ ‡ URL
            actions: åŠ¨ä½œåˆ—è¡¨ï¼Œæ”¯æŒçš„åŠ¨ä½œç±»å‹ï¼š
                - {"type": "click", "selector": "#button"}
                - {"type": "fill", "selector": "#input", "value": "text"}
                - {"type": "wait", "seconds": 2}
                - {"type": "scroll"}
                - {"type": "hover", "selector": ".menu"}
            wait_between: åŠ¨ä½œä¹‹é—´çš„ç­‰å¾…æ—¶é—´

        Returns:
            SiteReport å¯¹è±¡

        ç¤ºä¾‹ï¼š
            report = brain.interact("https://example.com", [
                {"type": "click", "selector": ".search-btn"},
                {"type": "wait", "seconds": 2},
            ])
        """
        logger.info(f"Interacting with: {url}, actions: {len(actions)}")
        report = self.collector.collect_with_interaction(
            url=url,
            actions=actions,
            wait_between_actions=wait_between,
        )

        self._last_report = report
        self._update_collected_data(report)

        logger.info(f"Interaction complete. Found {len(report.api_requests)} API requests")
        return report

    # ==================== ç¬¬äºŒé˜¶æ®µï¼šAPI è°ƒç”¨ ====================

    def call_api(
        self,
        url: str,
        method: Literal["GET", "POST", "PUT", "DELETE"] = "GET",
        headers: dict[str, str] | None = None,
        params: dict[str, Any] | None = None,
        json_body: dict[str, Any] | None = None,
        form_body: dict[str, Any] | None = None,
        use_collected_cookies: bool = True,
        timeout: float = 30.0,
        retry: bool = True,
    ) -> ApiCallResult:
        """
        ç›´æ¥è°ƒç”¨ APIï¼ˆå¸¦æ™ºèƒ½é‡è¯•ï¼‰

        å½“ AI åˆ†æå‡º API ç»“æ„åï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨ã€‚
        å†…ç½®æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼Œè‡ªåŠ¨å¤„ç†ä¸´æ—¶é”™è¯¯ã€‚

        Args:
            url: API URL
            method: è¯·æ±‚æ–¹æ³•
            headers: è¯·æ±‚å¤´
            params: URL å‚æ•°
            json_body: JSON è¯·æ±‚ä½“
            form_body: è¡¨å•è¯·æ±‚ä½“
            use_collected_cookies: æ˜¯å¦ä½¿ç”¨ä¹‹å‰æ”¶é›†çš„ Cookie
            timeout: è¶…æ—¶æ—¶é—´
            retry: æ˜¯å¦å¯ç”¨æ™ºèƒ½é‡è¯•

        Returns:
            ApiCallResult å¯¹è±¡

        ç¤ºä¾‹ï¼š
            result = brain.call_api(
                url="https://api.example.com/data",
                method="POST",
                headers={"Content-Type": "application/json"},
                json_body={"page": 1, "size": 20},
            )
            print(result.to_ai_summary())
        """
        logger.info(f"Calling API: {method} {url}")
        self._request_count += 1

        # æ„å»ºè¯·æ±‚å¤´
        final_headers = self._build_headers(headers, use_collected_cookies)

        # æ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦é‡è¯•ï¼‰
        max_attempts = self.retry_config.max_retries + 1 if retry else 1
        last_error = None
        retries = 0

        for attempt in range(max_attempts):
            start_time = time.time()
            try:
                result = self._execute_api_call(
                    url, method, final_headers, params, json_body, form_body, timeout
                )
                result.response_time_ms = (time.time() - start_time) * 1000
                result.retries = retries

                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
                if result.success:
                    return result

                if result.status_code in self.retry_config.retry_on_status:
                    if attempt < max_attempts - 1:
                        delay = self._calculate_retry_delay(attempt)
                        logger.warning(
                            f"Request failed with {result.status_code}, "
                            f"retrying in {delay:.1f}s (attempt {attempt + 1}/{max_attempts})"
                        )
                        time.sleep(delay)
                        retries += 1
                        continue

                return result

            except Exception as e:
                last_error = str(e)
                self._error_count += 1

                if attempt < max_attempts - 1:
                    delay = self._calculate_retry_delay(attempt)
                    logger.warning(
                        f"Request error: {e}, "
                        f"retrying in {delay:.1f}s (attempt {attempt + 1}/{max_attempts})"
                    )
                    time.sleep(delay)
                    retries += 1
                    continue

        return ApiCallResult(
            success=False,
            status_code=None,
            headers={},
            body=None,
            error=last_error,
            retries=retries,
            response_time_ms=0,
        )

    def _build_headers(
        self,
        custom_headers: dict[str, str] | None,
        use_collected_cookies: bool,
    ) -> dict[str, str]:
        """æ„å»ºè¯·æ±‚å¤´"""
        final_headers = {}

        # æ·»åŠ æ”¶é›†çš„ Cookie
        if use_collected_cookies and self._collected_cookies:
            cookie_str = "; ".join(f"{k}={v}" for k, v in self._collected_cookies.items())
            final_headers["Cookie"] = cookie_str

        # æ·»åŠ æ”¶é›†çš„å…¶ä»–å¤´
        if self._collected_headers:
            final_headers.update(self._collected_headers)

        # æ·»åŠ è‡ªå®šä¹‰å¤´
        if custom_headers:
            final_headers.update(custom_headers)

        return final_headers

    def _execute_api_call(
        self,
        url: str,
        method: str,
        headers: dict[str, str],
        params: dict[str, Any] | None,
        json_body: dict[str, Any] | None,
        form_body: dict[str, Any] | None,
        timeout: float,
    ) -> ApiCallResult:
        """æ‰§è¡Œå•æ¬¡ API è°ƒç”¨"""
        proxy = self.proxy_manager.get_httpx_proxy()

        with httpx.Client(proxy=proxy, timeout=timeout, follow_redirects=True) as client:
            response = client.request(
                method=method,
                url=url,
                headers=headers,
                params=params,
                json=json_body,
                data=form_body,
            )

            # è§£æå“åº”
            body: str | dict | list | None = None
            content_type = response.headers.get("content-type", "")

            if "json" in content_type:
                try:
                    body = response.json()
                except:
                    body = response.text
            else:
                body = response.text

            return ApiCallResult(
                success=response.status_code < 400,
                status_code=response.status_code,
                headers=dict(response.headers),
                body=body,
            )

    def _calculate_retry_delay(self, attempt: int) -> float:
        """è®¡ç®—é‡è¯•å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ + æŠ–åŠ¨ï¼‰"""
        delay = self.retry_config.base_delay * (self.retry_config.exponential_base ** attempt)
        delay = min(delay, self.retry_config.max_delay)

        if self.retry_config.jitter:
            delay = delay * (0.5 + random.random())

        return delay

    def batch_call_api(
        self,
        requests: list[dict[str, Any]],
        delay_between: float = 1.0,
        stop_on_error: bool = False,
    ) -> list[ApiCallResult]:
        """
        æ‰¹é‡è°ƒç”¨ API

        Args:
            requests: è¯·æ±‚åˆ—è¡¨ï¼Œæ¯ä¸ªè¯·æ±‚æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« call_api çš„å‚æ•°
            delay_between: è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
            stop_on_error: é‡åˆ°é”™è¯¯æ˜¯å¦åœæ­¢

        Returns:
            ç»“æœåˆ—è¡¨

        ç¤ºä¾‹ï¼š
            results = brain.batch_call_api([
                {"url": "https://api.example.com/page/1"},
                {"url": "https://api.example.com/page/2"},
                {"url": "https://api.example.com/page/3"},
            ])
        """
        results = []
        for i, req in enumerate(requests):
            logger.info(f"Batch request {i + 1}/{len(requests)}")

            result = self.call_api(**req)
            results.append(result)

            if not result.success and stop_on_error:
                logger.warning(f"Stopping batch due to error: {result.error}")
                break

            if i < len(requests) - 1:
                time.sleep(delay_between)

        return results

    # ==================== ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®æŠ“å– ====================

    def scrape_page(
        self,
        url: str,
        max_pages: int = 1,
    ) -> ScrapeResult:
        """
        æŠ“å–é¡µé¢æ•°æ®

        ä½¿ç”¨æ™ºèƒ½æå–ä»é¡µé¢ DOM ä¸­æå–æ•°æ®ã€‚

        Args:
            url: ç›®æ ‡ URL
            max_pages: æœ€å¤§ç¿»é¡µæ•°

        Returns:
            ScrapeResult å¯¹è±¡
        """
        return self.scraper.scrape(url, max_pages=max_pages)

    def scrape_with_selector(
        self,
        url: str,
        item_selector: str,
        fields: list[dict[str, str]],
        max_pages: int = 1,
    ) -> ScrapeResult:
        """
        ä½¿ç”¨è‡ªå®šä¹‰é€‰æ‹©å™¨æŠ“å–

        Args:
            url: ç›®æ ‡ URL
            item_selector: åˆ—è¡¨é¡¹é€‰æ‹©å™¨
            fields: å­—æ®µé…ç½®åˆ—è¡¨
            max_pages: æœ€å¤§ç¿»é¡µæ•°

        Returns:
            ScrapeResult å¯¹è±¡

        ç¤ºä¾‹ï¼š
            result = brain.scrape_with_selector(
                url="https://example.com/products",
                item_selector=".product-card",
                fields=[
                    {"name": "title", "selector": ".title", "attr": "text"},
                    {"name": "price", "selector": ".price", "attr": "text"},
                ],
            )
        """
        return self.scraper.scrape_with_selector(
            url=url,
            item_selector=item_selector,
            fields=fields,
            max_pages=max_pages,
        )

    # ==================== å·¥å…·æ–¹æ³• ====================

    def get_last_report(self) -> SiteReport | None:
        """è·å–ä¸Šæ¬¡æ”¶é›†çš„æŠ¥å‘Š"""
        return self._last_report

    def get_collected_cookies(self) -> dict[str, str]:
        """è·å–æ”¶é›†åˆ°çš„ Cookie"""
        return self._collected_cookies.copy()

    def set_cookies(self, cookies: dict[str, str]) -> None:
        """æ‰‹åŠ¨è®¾ç½® Cookie"""
        self._collected_cookies.update(cookies)

    def set_headers(self, headers: dict[str, str]) -> None:
        """æ‰‹åŠ¨è®¾ç½®è¯·æ±‚å¤´"""
        self._collected_headers.update(headers)

    def save_report(self, report: SiteReport, filename: str | None = None) -> Path:
        """
        ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶

        Args:
            report: SiteReport å¯¹è±¡
            filename: æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"report_{timestamp}"

        output_path = self.config.data_dir / f"{filename}.json"
        output_path.write_text(report.to_json(), encoding="utf-8")

        # ä¿å­˜æˆªå›¾
        if report.screenshot_b64:
            import base64
            screenshot_path = self.config.data_dir / f"{filename}_screenshot.png"
            screenshot_path.write_bytes(base64.b64decode(report.screenshot_b64))

        logger.info(f"Report saved: {output_path}")
        return output_path

    def export_data(self, data: list[dict], filename: str, format: Literal["json", "csv"] = "json") -> Path:
        """å¯¼å‡ºæ•°æ®"""
        return self.scraper.export_data(data, filename, format)

    def _update_collected_data(self, report: SiteReport) -> None:
        """ä»æŠ¥å‘Šä¸­æ›´æ–°æ”¶é›†çš„æ•°æ®"""
        # æ›´æ–° Cookie
        for cookie in report.cookies:
            name = cookie.get("name")
            value = cookie.get("value")
            if name and value:
                self._collected_cookies[name] = value

        # ä» API è¯·æ±‚ä¸­æå–å¸¸è§è¯·æ±‚å¤´
        for req in report.api_requests:
            for header in ["User-Agent", "Referer", "Origin", "X-Requested-With"]:
                if header in req.headers and header not in self._collected_headers:
                    self._collected_headers[header] = req.headers[header]

    # ==================== çŠ¶æ€å’Œç»Ÿè®¡ ====================

    def get_stats(self) -> dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "request_count": self._request_count,
            "error_count": self._error_count,
            "success_rate": (
                (self._request_count - self._error_count) / self._request_count * 100
                if self._request_count > 0
                else 0
            ),
            "cookies_collected": len(self._collected_cookies),
            "headers_collected": len(self._collected_headers),
            "last_analysis": self._last_analysis.site_name if self._last_analysis else None,
        }

    def get_status_report(self, use_emoji: bool = True) -> str:
        """
        è·å–çŠ¶æ€æŠ¥å‘Šï¼ˆAI å‹å¥½æ ¼å¼ï¼‰

        Args:
            use_emoji: æ˜¯å¦ä½¿ç”¨ emojiï¼ˆWindows ç»ˆç«¯å¯èƒ½ä¸æ”¯æŒï¼‰
        """
        stats = self.get_stats()

        # emoji æˆ– ASCII æ›¿ä»£
        e = {
            "brain": "ğŸ§ " if use_emoji else "[Brain]",
            "stats": "ğŸ“Š" if use_emoji else "[Stats]",
            "cookie": "ğŸª" if use_emoji else "[Cookie]",
            "search": "ğŸ”" if use_emoji else "[Search]",
        }

        lines = [
            f"# {e['brain']} Brain çŠ¶æ€æŠ¥å‘Š",
            "",
            f"## {e['stats']} ç»Ÿè®¡æ•°æ®",
            f"- è¯·æ±‚æ¬¡æ•°: {stats['request_count']}",
            f"- é”™è¯¯æ¬¡æ•°: {stats['error_count']}",
            f"- æˆåŠŸç‡: {stats['success_rate']:.1f}%",
            "",
            f"## {e['cookie']} æ”¶é›†çš„æ•°æ®",
            f"- Cookie æ•°é‡: {stats['cookies_collected']}",
            f"- Header æ•°é‡: {stats['headers_collected']}",
        ]

        if stats['last_analysis']:
            lines.extend([
                "",
                f"## {e['search']} ä¸Šæ¬¡åˆ†æ",
                f"- ç½‘ç«™: {stats['last_analysis']}",
                f"- åçˆ¬ç­‰çº§: {self._last_analysis.anti_scrape_level}",
                f"- æ¨èæ–¹æ¡ˆ: {self._last_analysis.recommended_approach}",
            ])

        return "\n".join(lines)

    def reset(self) -> None:
        """é‡ç½®çŠ¶æ€"""
        self._last_report = None
        self._last_analysis = None
        self._collected_cookies.clear()
        self._collected_headers.clear()
        self._request_count = 0
        self._error_count = 0
        logger.info("Brain state reset")

    def get_last_analysis(self) -> SiteAnalysis | None:
        """è·å–ä¸Šæ¬¡çš„æ™ºèƒ½åˆ†æç»“æœ"""
        return self._last_analysis

    # ==================== åŸºç¡€è®¾æ–½è¯„ä¼°ï¼ˆè‡ªåŠ¨å»ºè®®ï¼‰ ====================

    def evaluate_requirements(
        self,
        url: str,
        target_count: int = 100,
        auto_analyze: bool = True,
    ) -> str:
        """
        è¯„ä¼°ä»»åŠ¡éœ€æ±‚å¹¶ç»™å‡ºåŸºç¡€è®¾æ–½å»ºè®®

        è¿™æ˜¯ Agent è‡ªä¸»è¯„ä¼°çš„æ ¸å¿ƒæ–¹æ³•ã€‚æ ¹æ®ç›®æ ‡URLå’Œæ•°æ®é‡ï¼Œ
        è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦ Cookieæ± ã€ä»£ç†æ± ã€ç­¾åæœåŠ¡ç­‰ã€‚

        Args:
            url: ç›®æ ‡URL
            target_count: ç›®æ ‡æ•°æ®é‡
            auto_analyze: æ˜¯å¦è‡ªåŠ¨è¿›è¡Œç½‘ç«™åˆ†æ

        Returns:
            äººç±»å¯è¯»çš„å»ºè®®æŠ¥å‘Š

        ç¤ºä¾‹ï¼š
            # Agent æ ¹æ®ç”¨æˆ·éœ€æ±‚è‡ªåŠ¨è¯„ä¼°
            report = brain.evaluate_requirements(
                url="https://jd.com/search?q=æ‰‹æœº",
                target_count=1000
            )
            print(report)  # è¾“å‡ºåŸºç¡€è®¾æ–½å»ºè®®
        """
        logger.info(f"Evaluating requirements for: {url}")

        # è·å–åˆ†æç»“æœ
        analysis_dict = None
        if auto_analyze and not self._last_analysis:
            try:
                analysis = self.smart_investigate(url)
                analysis_dict = {
                    "anti_scrape_level": analysis.anti_scrape_level,
                    "signatures_detected": [s.param_name for s in analysis.signature_params],
                    "cookies_detected": list(self._collected_cookies.keys()),
                }
            except Exception as e:
                logger.warning(f"Auto-analyze failed: {e}")
        elif self._last_analysis:
            analysis_dict = {
                "anti_scrape_level": self._last_analysis.anti_scrape_level,
                "signatures_detected": [s.param_name for s in self._last_analysis.signature_params],
                "cookies_detected": list(self._collected_cookies.keys()),
            }

        # è¯„ä¼°åŸºç¡€è®¾æ–½éœ€æ±‚
        recommendations = self.infra_advisor.evaluate(
            url=url,
            target_count=target_count,
            analysis_result=analysis_dict,
        )

        return recommendations["summary"]

    def get_cookie_pool(self, domain: str) -> CookiePool:
        """è·å–æŒ‡å®šåŸŸåçš„Cookieæ± """
        return self.cookie_pool_manager.get_pool(domain)

    async def prepare_cookie_pool(
        self,
        url: str,
        count: int = 10,
        wait_seconds: float = 3.0,
    ) -> int:
        """
        ä¸ºç›®æ ‡URLå‡†å¤‡Cookieæ± ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰

        Args:
            url: ç›®æ ‡URL
            count: è¦ç”Ÿæˆçš„Cookieæ•°é‡
            wait_seconds: æ¯æ¬¡è®¿é—®çš„ç­‰å¾…æ—¶é—´

        Returns:
            æˆåŠŸç”Ÿæˆçš„æ•°é‡
        """
        from urllib.parse import urlparse
        domain = urlparse(url).netloc.replace("www.", "")

        pool = self.cookie_pool_manager.get_pool(domain)
        return await pool.generate(count=count, url=url, wait_seconds=wait_seconds)

    def use_pool_cookie(self, url: str) -> bool:
        """
        ä»Cookieæ± è·å–Cookieå¹¶åº”ç”¨

        Args:
            url: ç›®æ ‡URL

        Returns:
            æ˜¯å¦æˆåŠŸè·å–å¹¶åº”ç”¨Cookie
        """
        from urllib.parse import urlparse
        domain = urlparse(url).netloc.replace("www.", "")

        pool = self.cookie_pool_manager.get_pool(domain)
        session = pool.get()

        if session:
            self._current_cookie_session = session
            self._collected_cookies.update(session.cookies)
            self._collected_headers["User-Agent"] = session.user_agent
            logger.info(f"Applied cookie from pool: {session.id}")
            return True

        return False

    def report_cookie_result(self, success: bool):
        """
        æŠ¥å‘ŠCookieä½¿ç”¨ç»“æœï¼ˆç”¨äºåé¦ˆå­¦ä¹ ï¼‰

        Args:
            success: æ˜¯å¦æˆåŠŸ
        """
        if self._current_cookie_session:
            from urllib.parse import urlparse
            domain = self._current_cookie_session.domain
            pool = self.cookie_pool_manager.get_pool(domain)

            if success:
                pool.mark_success(self._current_cookie_session.id)
            else:
                pool.mark_failed(self._current_cookie_session.id)

            self._current_cookie_session = None

    def cookie_pool_stats(self, url: str) -> dict:
        """è·å–Cookieæ± ç»Ÿè®¡"""
        from urllib.parse import urlparse
        domain = urlparse(url).netloc.replace("www.", "")
        pool = self.cookie_pool_manager.get_pool(domain)
        return pool.stats()


# ==================== ä¾¿æ·å‡½æ•° ====================

def quick_investigate(url: str) -> str:
    """
    å¿«é€Ÿè°ƒæŸ¥ç½‘ç«™å¹¶è¿”å›æ‘˜è¦

    è¿™æ˜¯æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼ï¼Œè¿”å›å¯è¯»çš„æ‘˜è¦ã€‚

    Args:
        url: ç›®æ ‡ URL

    Returns:
        æ‘˜è¦å­—ç¬¦ä¸²ï¼Œå¯ç›´æ¥ç”± AI åˆ†æ
    """
    brain = Brain(AgentConfig(proxy_enabled=False, headless=True))
    report = brain.investigate(url)
    return report.summary_for_ai()


def quick_smart_investigate(url: str) -> str:
    """
    å¿«é€Ÿæ™ºèƒ½è°ƒæŸ¥ï¼ˆæ¨èï¼‰

    è¿”å›å®Œæ•´çš„ AI åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
    - ç½‘ç«™è¯†åˆ«
    - åçˆ¬è¯„ä¼°
    - åŠ å¯†åˆ†æ
    - æ“ä½œå»ºè®®

    Args:
        url: ç›®æ ‡ URL

    Returns:
        AI å‹å¥½çš„å®Œæ•´åˆ†ææŠ¥å‘Š
    """
    brain = Brain(AgentConfig(proxy_enabled=False, headless=True))
    analysis = brain.smart_investigate(url)
    return analysis.to_ai_report()


def quick_scrape(url: str, max_pages: int = 3) -> list[dict]:
    """
    å¿«é€ŸæŠ“å–æ•°æ®

    Args:
        url: ç›®æ ‡ URL
        max_pages: æœ€å¤§ç¿»é¡µæ•°

    Returns:
        æ•°æ®åˆ—è¡¨
    """
    brain = Brain(AgentConfig(proxy_enabled=False, headless=True))
    result = brain.scrape_page(url, max_pages=max_pages)
    return result.data if result.success else []


def get_site_info(url: str) -> dict:
    """
    è·å–ç½‘ç«™é¢„è®¾ä¿¡æ¯

    å¿«é€ŸæŸ¥çœ‹ç½‘ç«™æ˜¯å¦æœ‰é¢„è®¾é…ç½®åŠç›¸å…³ä¿¡æ¯ã€‚

    Args:
        url: URL æˆ–åŸŸå

    Returns:
        é¢„è®¾ä¿¡æ¯å­—å…¸
    """
    return get_preset_info(url)
