"""
æ™ºèƒ½åˆ†ææ¨¡å—

è‡ªåŠ¨åˆ†æç½‘ç«™ç‰¹å¾ï¼Œç»™å‡ºï¼š
- ç½‘ç«™ç±»å‹è¯†åˆ«
- åçˆ¬å¼ºåº¦è¯„ä¼°
- åŠ å¯†å‚æ•°æ·±åº¦åˆ†æ
- å»ºè®®çš„æŠ“å–ç­–ç•¥
- ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®

è¿™ä¸ªæ¨¡å—çš„è¾“å‡ºæ˜¯ä¸“é—¨ä¸º AI è®¾è®¡çš„ï¼Œä¾¿äº Claude ç†è§£å’Œå†³ç­–ã€‚
"""

from __future__ import annotations

import re
import json
from dataclasses import dataclass, field
from typing import Any, Literal
from urllib.parse import urlparse


@dataclass
class SignatureAnalysis:
    """ç­¾åå‚æ•°åˆ†æç»“æœ"""
    param_name: str
    param_value: str
    pattern_type: Literal["timestamp", "hash", "token", "nonce", "unknown"]
    length: int
    charset: str  # "hex", "base64", "numeric", "alphanumeric", "mixed"
    appears_dynamic: bool  # æ˜¯å¦çœ‹èµ·æ¥æ˜¯åŠ¨æ€ç”Ÿæˆçš„
    possible_algorithm: str | None  # å¯èƒ½çš„ç®—æ³•
    confidence: float  # ç½®ä¿¡åº¦ 0-1
    notes: str


@dataclass
class SiteAnalysis:
    """ç½‘ç«™æ·±åº¦åˆ†æç»“æœ"""
    # åŸºæœ¬ä¿¡æ¯
    url: str
    domain: str
    site_type: str  # "ecommerce", "social", "news", "video", "unknown"
    site_name: str | None  # è¯†åˆ«å‡ºçš„ç½‘ç«™åç§°

    # åçˆ¬è¯„ä¼°
    anti_scrape_level: Literal["low", "medium", "high", "extreme"]
    anti_scrape_features: list[str]  # æ£€æµ‹åˆ°çš„åçˆ¬ç‰¹å¾

    # ç™»å½•çŠ¶æ€
    login_required: bool
    login_detected: bool  # æ˜¯å¦æ£€æµ‹åˆ°å·²ç™»å½•
    login_suggestion: str | None

    # API åˆ†æ
    api_endpoints: list[dict[str, Any]]
    main_data_api: dict[str, Any] | None  # ä¸»è¦æ•°æ®æ¥å£

    # ç­¾ååˆ†æ
    signature_params: list[SignatureAnalysis]
    encryption_complexity: Literal["none", "simple", "moderate", "complex"]

    # å»ºè®®
    recommended_approach: str  # "direct_api", "browser_scrape", "hybrid", "need_reverse"
    next_steps: list[str]  # å»ºè®®çš„ä¸‹ä¸€æ­¥æ“ä½œ
    warnings: list[str]  # è­¦å‘Šä¿¡æ¯

    # é¢„è®¾åŒ¹é…
    matched_preset: str | None  # åŒ¹é…åˆ°çš„é¢„è®¾é…ç½®åç§°

    def to_ai_report(self, use_emoji: bool = False) -> str:
        """
        ç”Ÿæˆ AI å‹å¥½çš„åˆ†ææŠ¥å‘Š

        Args:
            use_emoji: æ˜¯å¦ä½¿ç”¨ emojiï¼ˆWindows ç»ˆç«¯å¯èƒ½ä¸æ”¯æŒï¼‰
        """
        # æ ¹æ® use_emoji é€‰æ‹©ç¬¦å·
        if use_emoji:
            e = {"search": "ğŸ”", "info": "ğŸ“‹", "shield": "ğŸ›¡ï¸", "lock": "ğŸ”",
                 "api": "ğŸ”Œ", "bulb": "ğŸ’¡", "warn": "âš ï¸", "yes": "âœ…", "no": "âŒ",
                 "low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸŸ ", "extreme": "ğŸ”´", "unknown": "âšª"}
        else:
            e = {"search": "[åˆ†æ]", "info": "[ä¿¡æ¯]", "shield": "[åçˆ¬]", "lock": "[ç™»å½•]",
                 "api": "[API]", "bulb": "[å»ºè®®]", "warn": "[æ³¨æ„]", "yes": "[æ˜¯]", "no": "[å¦]",
                 "low": "[ä½]", "medium": "[ä¸­]", "high": "[é«˜]", "extreme": "[æé«˜]", "unknown": "[-]"}

        lines = [
            f"# {e['search']} ç½‘ç«™æ™ºèƒ½åˆ†ææŠ¥å‘Š",
            "",
            f"## {e['info']} åŸºæœ¬ä¿¡æ¯",
            f"- **åŸŸå**: {self.domain}",
            f"- **ç½‘ç«™ç±»å‹**: {self.site_type}",
            f"- **è¯†åˆ«ä¸º**: {self.site_name or 'æœªçŸ¥ç½‘ç«™'}",
        ]

        if self.matched_preset:
            lines.append(f"- **åŒ¹é…é¢„è®¾**: {e['yes']} {self.matched_preset}")
        lines.append("")

        # åçˆ¬è¯„ä¼°
        level_mark = e.get(self.anti_scrape_level, e['unknown'])
        lines.extend([
            f"## {e['shield']} åçˆ¬è¯„ä¼°",
            f"- **é˜²æŠ¤ç­‰çº§**: {level_mark} {self.anti_scrape_level.upper()}",
        ])
        if self.anti_scrape_features:
            lines.append("- **æ£€æµ‹åˆ°çš„é˜²æŠ¤æªæ–½**:")
            for feature in self.anti_scrape_features:
                lines.append(f"  - {feature}")
        lines.append("")

        # ç™»å½•çŠ¶æ€
        login_status = e['yes'] if self.login_detected else e['no']
        lines.extend([
            f"## {e['lock']} ç™»å½•çŠ¶æ€",
            f"- **éœ€è¦ç™»å½•**: {'æ˜¯' if self.login_required else 'å¦'}",
            f"- **å½“å‰çŠ¶æ€**: {'å·²ç™»å½• ' + login_status if self.login_detected else 'æœªç™»å½• ' + e['no']}",
        ])
        if self.login_suggestion:
            lines.append(f"- **å»ºè®®**: {self.login_suggestion}")
        lines.append("")

        # API åˆ†æ
        lines.append(f"## {e['api']} API åˆ†æ")
        if self.main_data_api:
            lines.extend([
                "### ä¸»è¦æ•°æ®æ¥å£",
                f"- **URL**: `{self.main_data_api.get('url', 'N/A')}`",
                f"- **æ–¹æ³•**: {self.main_data_api.get('method', 'GET')}",
                f"- **å‚æ•°**: {json.dumps(self.main_data_api.get('params', []), ensure_ascii=False)}",
            ])
        lines.append(f"\nå…±å‘ç° **{len(self.api_endpoints)}** ä¸ª API ç«¯ç‚¹")
        lines.append("")

        # ç­¾ååˆ†æ
        lines.extend([
            f"## {e['lock']} åŠ å¯†/ç­¾ååˆ†æ",
            f"- **å¤æ‚åº¦**: {self.encryption_complexity}",
        ])
        if self.signature_params:
            lines.append("- **å¯ç–‘å‚æ•°**:")
            for sig in self.signature_params:
                lines.append(f"  - `{sig.param_name}`: {sig.pattern_type}")
                if sig.possible_algorithm:
                    lines.append(f"    å¯èƒ½ç®—æ³•: {sig.possible_algorithm}")
                lines.append(f"    ç½®ä¿¡åº¦: {sig.confidence:.0%}")
        else:
            lines.append(f"- æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„åŠ å¯†å‚æ•° {e['yes']}")
        lines.append("")

        # å»ºè®®
        approach_desc = {
            "direct_api": "ç›´æ¥è°ƒç”¨ APIï¼ˆæœ€é«˜æ•ˆï¼‰",
            "browser_scrape": "æµè§ˆå™¨æ¸²æŸ“æŠ“å–",
            "hybrid": "æ··åˆæ¨¡å¼ï¼ˆæµè§ˆå™¨è·å–ç­¾å + API æŠ“å–ï¼‰",
            "need_reverse": "éœ€è¦é€†å‘åˆ†æåŠ å¯†ç®—æ³•",
        }
        lines.extend([
            f"## {e['bulb']} å»ºè®®",
            f"### æ¨èæ–¹æ¡ˆ: {approach_desc.get(self.recommended_approach, self.recommended_approach)}",
            "",
            "### ä¸‹ä¸€æ­¥æ“ä½œ:",
        ])
        for i, step in enumerate(self.next_steps, 1):
            lines.append(f"{i}. {step}")

        if self.warnings:
            lines.extend([
                "",
                f"### {e['warn']} æ³¨æ„äº‹é¡¹:",
            ])
            for warning in self.warnings:
                lines.append(f"- {warning}")

        return "\n".join(lines)


class SmartAnalyzer:
    """æ™ºèƒ½åˆ†æå™¨"""

    # å¸¸è§ç½‘ç«™è¯†åˆ«è§„åˆ™
    SITE_PATTERNS = {
        "jd.com": {"name": "äº¬ä¸œ", "type": "ecommerce", "anti_level": "extreme"},
        "taobao.com": {"name": "æ·˜å®", "type": "ecommerce", "anti_level": "extreme"},
        "tmall.com": {"name": "å¤©çŒ«", "type": "ecommerce", "anti_level": "extreme"},
        "pinduoduo.com": {"name": "æ‹¼å¤šå¤š", "type": "ecommerce", "anti_level": "high"},
        "1688.com": {"name": "1688", "type": "ecommerce", "anti_level": "high"},
        "amazon": {"name": "äºšé©¬é€Š", "type": "ecommerce", "anti_level": "high"},
        "douyin.com": {"name": "æŠ–éŸ³", "type": "video", "anti_level": "extreme"},
        "kuaishou.com": {"name": "å¿«æ‰‹", "type": "video", "anti_level": "extreme"},
        "xiaohongshu.com": {"name": "å°çº¢ä¹¦", "type": "social", "anti_level": "high"},
        "weibo.com": {"name": "å¾®åš", "type": "social", "anti_level": "medium"},
        "zhihu.com": {"name": "çŸ¥ä¹", "type": "social", "anti_level": "medium"},
        "bilibili.com": {"name": "Bç«™", "type": "video", "anti_level": "medium"},
        "dangdang.com": {"name": "å½“å½“", "type": "ecommerce", "anti_level": "low"},
        "suning.com": {"name": "è‹å®", "type": "ecommerce", "anti_level": "medium"},
    }

    # åŠ å¯†å‚æ•°ç‰¹å¾
    SIGNATURE_PATTERNS = {
        "h5st": {"type": "hash", "algorithm": "äº¬ä¸œ H5ST ç­¾å", "complexity": "complex"},
        "sign": {"type": "hash", "algorithm": "é€šç”¨ç­¾å", "complexity": "moderate"},
        "_sign": {"type": "hash", "algorithm": "é€šç”¨ç­¾å", "complexity": "moderate"},
        "signature": {"type": "hash", "algorithm": "é€šç”¨ç­¾å", "complexity": "moderate"},
        "token": {"type": "token", "algorithm": "Token è®¤è¯", "complexity": "simple"},
        "timestamp": {"type": "timestamp", "algorithm": None, "complexity": "simple"},
        "_t": {"type": "timestamp", "algorithm": None, "complexity": "simple"},
        "nonce": {"type": "nonce", "algorithm": "éšæœºæ•°", "complexity": "simple"},
        "appkey": {"type": "token", "algorithm": "App Key", "complexity": "simple"},
    }

    def analyze(self, report) -> SiteAnalysis:
        """
        æ·±åº¦åˆ†ææ”¶é›†åˆ°çš„ä¿¡æ¯

        Args:
            report: SiteReport å¯¹è±¡ï¼ˆæ¥è‡ª InfoCollectorï¼‰

        Returns:
            SiteAnalysis å¯¹è±¡
        """
        from .collector import SiteReport

        parsed = urlparse(report.url)
        domain = parsed.netloc.lower()

        # è¯†åˆ«ç½‘ç«™
        site_info = self._identify_site(domain)

        # åˆ†æåçˆ¬ç‰¹å¾
        anti_scrape = self._analyze_anti_scrape(report, site_info)

        # åˆ†æç™»å½•çŠ¶æ€
        login_info = self._analyze_login(report)

        # åˆ†æ API
        api_analysis = self._analyze_apis(report)

        # åˆ†æç­¾åå‚æ•°
        signature_analysis = self._analyze_signatures(report)

        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(
            site_info, anti_scrape, login_info, api_analysis, signature_analysis
        )

        return SiteAnalysis(
            url=report.url,
            domain=domain,
            site_type=site_info.get("type", "unknown"),
            site_name=site_info.get("name"),
            anti_scrape_level=anti_scrape["level"],
            anti_scrape_features=anti_scrape["features"],
            login_required=login_info["required"],
            login_detected=login_info["detected"],
            login_suggestion=login_info["suggestion"],
            api_endpoints=api_analysis["endpoints"],
            main_data_api=api_analysis["main_api"],
            signature_params=signature_analysis["params"],
            encryption_complexity=signature_analysis["complexity"],
            recommended_approach=recommendations["approach"],
            next_steps=recommendations["steps"],
            warnings=recommendations["warnings"],
            matched_preset=site_info.get("preset"),
        )

    def _identify_site(self, domain: str) -> dict:
        """è¯†åˆ«ç½‘ç«™"""
        for pattern, info in self.SITE_PATTERNS.items():
            if pattern in domain:
                return {**info, "preset": info["name"]}
        return {"name": None, "type": "unknown", "anti_level": "medium", "preset": None}

    def _analyze_anti_scrape(self, report, site_info: dict) -> dict:
        """åˆ†æåçˆ¬ç‰¹å¾"""
        features = []
        level = site_info.get("anti_level", "medium")

        # æ£€æŸ¥ Cookie æ•°é‡å’Œç±»å‹
        cookie_names = [c.get("name", "").lower() for c in report.cookies]

        # å¸¸è§åçˆ¬ Cookie
        anti_cookies = ["__jda", "__jdb", "__jdc", "__jdu", "_tb_token_", "cna", "mtop"]
        for ac in anti_cookies:
            if any(ac in cn for cn in cookie_names):
                features.append(f"æ£€æµ‹åˆ°åçˆ¬ Cookie: {ac}")

        # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯ç ç›¸å…³è¯·æ±‚
        for req in report.all_requests:
            url_lower = req.url.lower()
            if any(kw in url_lower for kw in ["captcha", "verify", "slider", "jschl"]):
                features.append("æ£€æµ‹åˆ°éªŒè¯ç /æ»‘å—è¯·æ±‚")
                level = "high" if level != "extreme" else level

        # æ£€æŸ¥åŠ å¯†å‚æ•°æ•°é‡
        if len(report.detected_encryption_params) > 3:
            features.append(f"æ£€æµ‹åˆ° {len(report.detected_encryption_params)} ä¸ªåŠ å¯†å‚æ•°")
            if level == "low":
                level = "medium"
            elif level == "medium":
                level = "high"

        # æ£€æŸ¥æ˜¯å¦æœ‰ WebSocket
        for req in report.all_requests:
            if "ws://" in req.url or "wss://" in req.url:
                features.append("ä½¿ç”¨ WebSocket é€šä¿¡")

        return {"level": level, "features": features}

    def _analyze_login(self, report) -> dict:
        """åˆ†æç™»å½•çŠ¶æ€"""
        result = {
            "required": False,
            "detected": False,
            "suggestion": None,
        }

        # æ£€æŸ¥ç™»å½•ç›¸å…³ Cookie
        login_cookies = ["token", "session", "user", "uid", "login", "auth"]
        cookie_names = [c.get("name", "").lower() for c in report.cookies]

        has_login_cookie = any(
            any(lc in cn for lc in login_cookies)
            for cn in cookie_names
        )

        if has_login_cookie:
            result["detected"] = True

        # æ£€æŸ¥é¡µé¢ä¸­æ˜¯å¦æœ‰ç™»å½•æç¤º
        html_lower = report.page_html.lower() if report.page_html else ""

        login_hints = ["è¯·ç™»å½•", "è¯·å…ˆç™»å½•", "login required", "sign in", "ç™»å½•å"]
        if any(hint in html_lower for hint in login_hints):
            result["required"] = True
            if not result["detected"]:
                result["suggestion"] = "å»ºè®®å…ˆæ‰‹åŠ¨ç™»å½•ï¼Œç„¶åä¿å­˜ Cookie å†è¿›è¡ŒæŠ“å–"

        return result

    def _analyze_apis(self, report) -> dict:
        """åˆ†æ API ç«¯ç‚¹"""
        endpoints = []
        main_api = None

        # æ•°æ® API çš„ç‰¹å¾
        data_keywords = ["list", "search", "query", "product", "item", "comment", "review", "data"]

        for req in report.api_requests:
            endpoint = {
                "url": req.url,
                "method": req.method,
                "params": list(req.query_params.keys()) + list(req.body_params.keys()),
                "has_signature": bool(req.suspicious_params),
            }
            endpoints.append(endpoint)

            # åˆ¤æ–­æ˜¯å¦æ˜¯ä¸»è¦æ•°æ®æ¥å£
            url_lower = req.url.lower()
            if any(kw in url_lower for kw in data_keywords):
                if main_api is None or len(endpoint["params"]) > len(main_api.get("params", [])):
                    main_api = endpoint

        return {"endpoints": endpoints, "main_api": main_api}

    def _analyze_signatures(self, report) -> list[SignatureAnalysis]:
        """æ·±åº¦åˆ†æç­¾åå‚æ•°"""
        params = []
        complexity = "none"

        all_suspicious = set()
        param_values = {}

        # æ”¶é›†æ‰€æœ‰å¯ç–‘å‚æ•°
        for req in report.api_requests:
            for param in req.suspicious_params:
                all_suspicious.add(param)
                # è·å–å‚æ•°å€¼
                value = req.query_params.get(param, [None])[0] or req.body_params.get(param)
                if value:
                    param_values[param] = str(value) if not isinstance(value, str) else value

        # åˆ†ææ¯ä¸ªå‚æ•°
        for param in all_suspicious:
            value = param_values.get(param, "")
            analysis = self._analyze_single_param(param, value)
            params.append(analysis)

            # æ›´æ–°å¤æ‚åº¦
            if analysis.pattern_type in ("hash",) and analysis.confidence > 0.7:
                if complexity in ("none", "simple"):
                    complexity = "moderate"
                if len(value) > 32:
                    complexity = "complex"

        # æ£€æŸ¥æ˜¯å¦æœ‰å·²çŸ¥çš„å¤æ‚ç­¾å
        for param in all_suspicious:
            param_lower = param.lower()
            if "h5st" in param_lower:
                complexity = "complex"

        return {"params": params, "complexity": complexity}

    def _analyze_single_param(self, name: str, value: str) -> SignatureAnalysis:
        """åˆ†æå•ä¸ªå‚æ•°"""
        name_lower = name.lower()
        value = value or ""

        # é»˜è®¤å€¼
        pattern_type = "unknown"
        charset = "mixed"
        possible_algorithm = None
        confidence = 0.5

        # æ£€æµ‹é•¿åº¦
        length = len(value)

        # æ£€æµ‹å­—ç¬¦é›†
        if re.match(r'^[0-9]+$', value):
            charset = "numeric"
            if length == 10 or length == 13:
                pattern_type = "timestamp"
                possible_algorithm = "Unix æ—¶é—´æˆ³"
                confidence = 0.9
        elif re.match(r'^[0-9a-fA-F]+$', value):
            charset = "hex"
            if length == 32:
                pattern_type = "hash"
                possible_algorithm = "MD5"
                confidence = 0.8
            elif length == 40:
                pattern_type = "hash"
                possible_algorithm = "SHA1"
                confidence = 0.8
            elif length == 64:
                pattern_type = "hash"
                possible_algorithm = "SHA256"
                confidence = 0.8
        elif re.match(r'^[0-9a-zA-Z+/=]+$', value):
            charset = "base64"
            if len(value) % 4 == 0:
                pattern_type = "token"
                possible_algorithm = "Base64 ç¼–ç "
                confidence = 0.7
        elif re.match(r'^[0-9a-zA-Z]+$', value):
            charset = "alphanumeric"

        # æ ¹æ®å‚æ•°åè°ƒæ•´
        if any(kw in name_lower for kw in ["sign", "signature", "hash"]):
            pattern_type = "hash"
            confidence = max(confidence, 0.8)
        elif any(kw in name_lower for kw in ["token", "auth", "key"]):
            pattern_type = "token"
            confidence = max(confidence, 0.7)
        elif any(kw in name_lower for kw in ["time", "_t", "ts"]):
            pattern_type = "timestamp"
            confidence = max(confidence, 0.7)
        elif any(kw in name_lower for kw in ["nonce", "rand"]):
            pattern_type = "nonce"
            confidence = max(confidence, 0.7)

        # æ£€æŸ¥æ˜¯å¦çœ‹èµ·æ¥æ˜¯åŠ¨æ€çš„
        appears_dynamic = (
            pattern_type in ("hash", "nonce") or
            (pattern_type == "timestamp" and length >= 10)
        )

        # ç”Ÿæˆè¯´æ˜
        notes = f"é•¿åº¦ {length}, å­—ç¬¦é›† {charset}"
        if possible_algorithm:
            notes += f", å¯èƒ½æ˜¯ {possible_algorithm}"

        return SignatureAnalysis(
            param_name=name,
            param_value=value[:50] + "..." if len(value) > 50 else value,
            pattern_type=pattern_type,
            length=length,
            charset=charset,
            appears_dynamic=appears_dynamic,
            possible_algorithm=possible_algorithm,
            confidence=confidence,
            notes=notes,
        )

    def _generate_recommendations(
        self,
        site_info: dict,
        anti_scrape: dict,
        login_info: dict,
        api_analysis: dict,
        signature_analysis: dict,
    ) -> dict:
        """ç”Ÿæˆå»ºè®®"""
        steps = []
        warnings = []
        approach = "browser_scrape"  # é»˜è®¤

        # æ ¹æ®åçˆ¬ç­‰çº§å†³å®šåŸºç¡€ç­–ç•¥
        if anti_scrape["level"] == "low":
            approach = "direct_api" if api_analysis["main_api"] else "browser_scrape"
        elif anti_scrape["level"] == "medium":
            approach = "browser_scrape"
        elif anti_scrape["level"] in ("high", "extreme"):
            if signature_analysis["complexity"] == "complex":
                approach = "need_reverse"
            else:
                approach = "hybrid"

        # ç”Ÿæˆæ­¥éª¤å»ºè®® (ä¸ä½¿ç”¨ emojiï¼Œé¿å… Windows ç»ˆç«¯ç¼–ç é—®é¢˜)
        if login_info["required"] and not login_info["detected"]:
            steps.append("[ç™»å½•] å…ˆæ‰‹åŠ¨ç™»å½•ç½‘ç«™ï¼Œç„¶åè°ƒç”¨ `brain.save_cookies()` ä¿å­˜ç™»å½•æ€")
            warnings.append("æ­¤ç½‘ç«™éœ€è¦ç™»å½•æ‰èƒ½è·å–å®Œæ•´æ•°æ®")

        if approach == "direct_api" and api_analysis["main_api"]:
            main_api = api_analysis["main_api"]
            steps.append(f"[API] ç›´æ¥è°ƒç”¨ API: `brain.call_api('{main_api['url']}')`")
            if main_api.get("has_signature"):
                steps.append("[æ³¨æ„] API éœ€è¦ç­¾åå‚æ•°ï¼Œå¯èƒ½éœ€è¦å…ˆä»æµè§ˆå™¨è·å–")

        elif approach == "browser_scrape":
            steps.append("[æµè§ˆå™¨] ä½¿ç”¨æµè§ˆå™¨æŠ“å–: `brain.scrape_page(url)`")
            steps.append("[é€‰æ‹©å™¨] æˆ–è€…æŒ‡å®šé€‰æ‹©å™¨: `brain.scrape_with_selector(url, selector, fields)`")

        elif approach == "hybrid":
            steps.append("[æ··åˆ] ä½¿ç”¨æ··åˆæ¨¡å¼:")
            steps.append("   1. `brain.interact()` è§¦å‘è¯·æ±‚å¹¶æ•è·ç­¾å")
            steps.append("   2. åˆ†æç­¾åå‚æ•°è§„å¾‹")
            steps.append("   3. `brain.call_api()` ç›´æ¥è°ƒç”¨ API")

        elif approach == "need_reverse":
            steps.append("[é€†å‘] æ­¤ç½‘ç«™åŠ å¯†è¾ƒå¤æ‚ï¼Œå»ºè®®:")
            steps.append("   1. ä½¿ç”¨ `brain.interact()` å¤šæ¬¡æ“ä½œï¼Œæ”¶é›†æ›´å¤šç­¾åæ ·æœ¬")
            steps.append("   2. åˆ†æç­¾åå‚æ•°çš„å˜åŒ–è§„å¾‹")
            steps.append("   3. è€ƒè™‘ä½¿ç”¨æµè§ˆå™¨ä½œä¸ºç­¾åæœåŠ¡å™¨ï¼ˆRPC æ¨¡å¼ï¼‰")
            warnings.append("æ­¤ç½‘ç«™ä½¿ç”¨å¤æ‚åŠ å¯†ï¼Œå¯èƒ½éœ€è¦é€†å‘åˆ†æ")

        # æ·»åŠ ä»£ç†å»ºè®®
        if anti_scrape["level"] in ("high", "extreme"):
            steps.append("[ä»£ç†] å»ºè®®å¯ç”¨ä»£ç†: `AgentConfig.for_kuaidaili(username, password)`")
            warnings.append("é«˜é˜²æŠ¤ç½‘ç«™å»ºè®®ä½¿ç”¨ä»£ç†é¿å… IP è¢«å°")

        # ç‰¹å®šç½‘ç«™å»ºè®®
        if site_info.get("name") == "äº¬ä¸œ":
            warnings.append("äº¬ä¸œä½¿ç”¨ H5ST ç­¾åï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†")
            steps.append("[æç¤º] å¯ä»¥ä½¿ç”¨é¡¹ç›®ä¸­çš„ `jd_rpc_signer` è·å–ç­¾å")

        return {"approach": approach, "steps": steps, "warnings": warnings}
