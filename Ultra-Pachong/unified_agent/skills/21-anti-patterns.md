# 21-anti-patterns.md - åé¢æ•™æåº“

## æ¨¡å—ç›®æ ‡

| ç›®æ ‡ | KPI | éªŒæ”¶æ ‡å‡† |
|------|-----|----------|
| ä»å¤±è´¥ä¸­å­¦ä¹  | åŒç±»é”™è¯¯é‡å¤ç‡ < 5% | ä¸é‡è¹ˆè¦†è¾™ |
| è¯†åˆ«å±é™©æ¨¡å¼ | é£é™©è¯†åˆ«ç‡ > 95% | æå‰é¢„è­¦ |
| æ˜ç¡®ç¦æ­¢è¡Œä¸º | è¿è§„æ“ä½œ 0 æ¬¡ | ç»å¯¹ä¸åš |

**æ ¸å¿ƒåŸåˆ™**ï¼š`ç»™å‡ºéœ€æ±‚ï¼Œå¿…é¡»å®Œæˆ` - çŸ¥é“ä»€ä¹ˆä¸èƒ½åšï¼Œæ‰èƒ½æ›´å¥½åœ°å®Œæˆä»»åŠ¡ã€‚

---

## ä¸€ã€ç»å¯¹ç¦æ­¢è¡Œä¸º (çº¢çº¿)

### ğŸš« RED-01: ä¼ªé€ æ•°æ®

**é”™è¯¯è¡Œä¸º**
```python
# âŒ é”™è¯¯: æ•°æ®ä¸ºç©ºæ—¶ä¼ªé€ æ•°æ®
def get_product_info(url):
    try:
        data = scrape(url)
        if not data:
            # ä¼ªé€ æ•°æ®å¡«å…… - ç»å¯¹ç¦æ­¢ï¼
            return {
                "title": "å•†å“æ ‡é¢˜",
                "price": "99.00",
                "stock": "æœ‰è´§"
            }
        return data
    except:
        # å‡ºé”™ä¹Ÿä¼ªé€  - ç»å¯¹ç¦æ­¢ï¼
        return {"title": "é»˜è®¤å•†å“", "price": "0"}
```

**ä¸ºä»€ä¹ˆé”™è¯¯**
```
1. æ•°æ®å¤±å»å¯ä¿¡åº¦ - ç”¨æˆ·æ— æ³•åŒºåˆ†çœŸå‡
2. è¯¯å¯¼å†³ç­– - åŸºäºå‡æ•°æ®åšå‡ºé”™è¯¯åˆ¤æ–­
3. æ— æ³•è¿½æº¯ - ä¸çŸ¥é“å“ªäº›æ˜¯çœŸå®æ•°æ®
4. è¿èƒŒè¯šä¿¡ - AIå¿…é¡»è¯šå®
```

**æ­£ç¡®åšæ³•**
```python
# âœ… æ­£ç¡®: æ˜ç¡®æ ‡è®°æ•°æ®çŠ¶æ€
def get_product_info(url):
    result = {
        "url": url,
        "status": "pending",
        "data": None,
        "error": None,
        "attempts": 0
    }

    try:
        data = scrape(url)
        if data:
            result["status"] = "success"
            result["data"] = data
        else:
            result["status"] = "empty"
            result["error"] = "æ•°æ®ä¸ºç©ºï¼Œé¡µé¢å¯èƒ½å·²ä¸‹æ¶æˆ–ç»“æ„å˜åŒ–"
    except Exception as e:
        result["status"] = "failed"
        result["error"] = str(e)

    result["attempts"] = 1
    result["timestamp"] = datetime.now().isoformat()

    return result
```

---

### ğŸš« RED-02: å¿½ç•¥é”™è¯¯é™é»˜ç»§ç»­

**é”™è¯¯è¡Œä¸º**
```python
# âŒ é”™è¯¯: æ•è·å¼‚å¸¸ä½†ä¸å¤„ç†
def batch_scrape(urls):
    results = []
    for url in urls:
        try:
            data = scrape(url)
            results.append(data)
        except:
            pass  # é™é»˜å¿½ç•¥ - å±é™©ï¼

    return results  # ç”¨æˆ·ä¸çŸ¥é“æœ‰å¤±è´¥
```

**ä¸ºä»€ä¹ˆé”™è¯¯**
```
1. æ•°æ®ä¸å®Œæ•´ - ç”¨æˆ·ä»¥ä¸ºå…¨éƒ¨æˆåŠŸ
2. é—®é¢˜è¢«æ©ç›– - æ— æ³•å‘ç°ç³»ç»Ÿé—®é¢˜
3. æ— æ³•ä¿®å¤ - ä¸çŸ¥é“å“ªäº›å¤±è´¥äº†
4. ç»Ÿè®¡å¤±çœŸ - æˆåŠŸç‡è™šé«˜
```

**æ­£ç¡®åšæ³•**
```python
# âœ… æ­£ç¡®: è®°å½•æ¯ä¸ªè¯·æ±‚çš„çŠ¶æ€
def batch_scrape(urls):
    results = {
        "success": [],
        "failed": [],
        "summary": {
            "total": len(urls),
            "success_count": 0,
            "fail_count": 0
        }
    }

    for url in urls:
        try:
            data = scrape(url)
            results["success"].append({"url": url, "data": data})
            results["summary"]["success_count"] += 1
        except Exception as e:
            results["failed"].append({
                "url": url,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })
            results["summary"]["fail_count"] += 1

            # è®°å½•åˆ°æ—¥å¿—
            logger.error(f"é‡‡é›†å¤±è´¥: {url}, é”™è¯¯: {e}")

    return results
```

---

### ğŸš« RED-03: æ— é™é‡è¯•

**é”™è¯¯è¡Œä¸º**
```python
# âŒ é”™è¯¯: æ²¡æœ‰é‡è¯•ä¸Šé™
def fetch_with_retry(url):
    while True:  # æ— é™å¾ªç¯ - å±é™©ï¼
        try:
            return requests.get(url)
        except:
            time.sleep(1)
            continue  # æ°¸è¿œé‡è¯•
```

**ä¸ºä»€ä¹ˆé”™è¯¯**
```
1. èµ„æºè€—å°½ - CPU/å†…å­˜æŒç»­å ç”¨
2. ä»»åŠ¡é˜»å¡ - å¡åœ¨ä¸€ä¸ªURLæ— æ³•ç»§ç»­
3. IPè¢«å° - æŒç»­è¯·æ±‚åŠ é€Ÿå°ç¦
4. æ— æ³•é€€å‡º - ç¨‹åºå¯èƒ½æ°¸è¿œæ— æ³•ç»“æŸ
```

**æ­£ç¡®åšæ³•**
```python
# âœ… æ­£ç¡®: æœ‰é™é‡è¯• + æŒ‡æ•°é€€é¿
def fetch_with_retry(url, max_retries=3, base_delay=1):
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            if attempt == max_retries - 1:
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                raise Exception(f"è¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {e}")

            delay = base_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
            logger.warning(f"é‡è¯• {attempt + 1}/{max_retries}, ç­‰å¾… {delay}s")
            time.sleep(delay)
```

---

### ğŸš« RED-04: ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯

**é”™è¯¯è¡Œä¸º**
```python
# âŒ é”™è¯¯: å¯†ç ç›´æ¥å†™åœ¨ä»£ç ä¸­
class Scraper:
    def __init__(self):
        self.username = "admin"
        self.password = "P@ssw0rd123"  # ç¡¬ç¼–ç å¯†ç  - å±é™©ï¼
        self.api_key = "sk-1234567890abcdef"  # ç¡¬ç¼–ç API Key

    def login(self):
        requests.post("/login", data={
            "user": self.username,
            "pass": self.password
        })
```

**ä¸ºä»€ä¹ˆé”™è¯¯**
```
1. ä»£ç æ³„éœ² = å‡­æ®æ³„éœ²
2. ç‰ˆæœ¬æ§åˆ¶ä¼šè®°å½•å†å²
3. æ— æ³•è½®æ¢å¯†ç 
4. å¤šç¯å¢ƒæ— æ³•åŒºåˆ†
```

**æ­£ç¡®åšæ³•**
```python
# âœ… æ­£ç¡®: ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶
import os
from dotenv import load_dotenv

load_dotenv()

class Scraper:
    def __init__(self):
        self.username = os.getenv("SCRAPER_USERNAME")
        self.password = os.getenv("SCRAPER_PASSWORD")
        self.api_key = os.getenv("API_KEY")

        if not all([self.username, self.password]):
            raise ValueError("ç¼ºå°‘å¿…è¦çš„å‡­æ®é…ç½®")

    def login(self):
        # å‡­æ®ä»ç¯å¢ƒå˜é‡è·å–
        ...
```

---

### ğŸš« RED-05: ç»•è¿‡è®¤è¯/æˆæƒéæ³•è®¿é—®

**é”™è¯¯è¡Œä¸º**
```python
# âŒ é”™è¯¯: å°è¯•ç»•è¿‡è®¤è¯
def access_admin_panel():
    # å°è¯•çŒœæµ‹ç®¡ç†å‘˜URL
    for path in ["/admin", "/administrator", "/wp-admin"]:
        response = requests.get(f"{base_url}{path}")
        if response.status_code == 200:
            return response  # éæ³•è®¿é—®

# âŒ é”™è¯¯: SQLæ³¨å…¥å°è¯•
def login(username, password):
    payload = f"' OR '1'='1"  # SQLæ³¨å…¥ - éæ³•ï¼
    requests.post("/login", data={"user": payload})
```

**ä¸ºä»€ä¹ˆé”™è¯¯**
```
1. è¿æ³•è¡Œä¸º - å¯èƒ½è§¦çŠ¯æ³•å¾‹
2. é“å¾·é—®é¢˜ - AIä¸åº”ååŠ©éæ³•æ´»åŠ¨
3. ä¿¡ä»»ç ´å - å¤±å»ç”¨æˆ·ä¿¡ä»»
4. é£é™©å·¨å¤§ - å¯èƒ½å¯¼è‡´ä¸¥é‡åæœ
```

**æ­£ç¡®åšæ³•**
```python
# âœ… æ­£ç¡®: åªè®¿é—®æˆæƒçš„èµ„æº
def access_data(url, credentials=None):
    # åªè®¿é—®æ˜ç¡®æˆæƒçš„URL
    if not is_authorized_url(url):
        raise UnauthorizedAccessError(f"æœªæˆæƒè®¿é—®: {url}")

    # ä½¿ç”¨åˆæ³•å‡­æ®
    if credentials:
        response = requests.get(url, auth=credentials)
    else:
        response = requests.get(url)

    return response
```

---

## äºŒã€å¸¸è§é”™è¯¯æ¨¡å¼ (é»„çº¿)

### âš ï¸ WARN-01: è¯·æ±‚é¢‘ç‡è¿‡é«˜

**é”™è¯¯è¡Œä¸º**
```python
# âš ï¸ è­¦å‘Š: æ— å»¶è¿Ÿæ‰¹é‡è¯·æ±‚
def scrape_all(urls):
    for url in urls:
        response = requests.get(url)  # æ— å»¶è¿Ÿ
        process(response)
    # å¯èƒ½ 1 ç§’å†…å‘é€ 100 ä¸ªè¯·æ±‚
```

**åæœ**
```
1. è§¦å‘é¢‘ç‡é™åˆ¶ (429)
2. IPè¢«ä¸´æ—¶æˆ–æ°¸ä¹…å°ç¦
3. æœåŠ¡å™¨è¿‡è½½
4. æ•°æ®ä¸å®Œæ•´
```

**æ­£ç¡®åšæ³•**
```python
# âœ… æ­£ç¡®: åˆç†çš„è¯·æ±‚é—´éš”
import random
import time

def scrape_all(urls, min_delay=1, max_delay=3):
    for url in urls:
        response = requests.get(url)
        process(response)

        # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)
```

**æœ€ä½³å®è·µ**
```
- æ™®é€šç½‘ç«™: 1-3 ç§’é—´éš”
- ä¸¥æ ¼ç½‘ç«™: 5-10 ç§’é—´éš”
- å‚è€ƒ robots.txt çš„ Crawl-delay
- è§‚å¯Ÿå“åº”æ—¶é—´ï¼ŒåŠ¨æ€è°ƒæ•´
```

---

### âš ï¸ WARN-02: ä¸æ£€æŸ¥å“åº”çŠ¶æ€

**é”™è¯¯è¡Œä¸º**
```python
# âš ï¸ è­¦å‘Š: å‡è®¾è¯·æ±‚æ€»æ˜¯æˆåŠŸ
def get_data(url):
    response = requests.get(url)
    return response.json()  # å¦‚æœä¸æ˜¯200æˆ–ä¸æ˜¯JSONä¼šå´©æºƒ
```

**åæœ**
```
1. ç¨‹åºå´©æºƒ (500/404ç­‰çŠ¶æ€ç )
2. è§£æé”™è¯¯ (å“åº”ä¸æ˜¯JSON)
3. è·å–é”™è¯¯æ•°æ® (é‡å®šå‘åˆ°é”™è¯¯é¡µ)
4. é€»è¾‘é”™è¯¯ (ç©ºå“åº”)
```

**æ­£ç¡®åšæ³•**
```python
# âœ… æ­£ç¡®: å®Œæ•´çš„å“åº”æ£€æŸ¥
def get_data(url):
    try:
        response = requests.get(url, timeout=30)

        # æ£€æŸ¥çŠ¶æ€ç 
        if response.status_code != 200:
            logger.error(f"é200å“åº”: {response.status_code}")
            return None

        # æ£€æŸ¥å†…å®¹ç±»å‹
        content_type = response.headers.get('Content-Type', '')
        if 'application/json' not in content_type:
            logger.error(f"éJSONå“åº”: {content_type}")
            return None

        # å°è¯•è§£æJSON
        try:
            data = response.json()
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            return None

        # æ£€æŸ¥æ•°æ®ç»“æ„
        if not data or 'error' in data:
            logger.error(f"å“åº”åŒ…å«é”™è¯¯: {data}")
            return None

        return data

    except requests.RequestException as e:
        logger.error(f"è¯·æ±‚å¼‚å¸¸: {e}")
        return None
```

---

### âš ï¸ WARN-03: ç¡¬ç¼–ç é€‰æ‹©å™¨

**é”™è¯¯è¡Œä¸º**
```python
# âš ï¸ è­¦å‘Š: è¿‡äºå…·ä½“çš„é€‰æ‹©å™¨
def get_price(html):
    soup = BeautifulSoup(html)
    # å¤ªè„†å¼± - ä»»ä½•ç»“æ„å˜åŒ–éƒ½ä¼šå¤±æ•ˆ
    price = soup.select_one(
        "div.container > div.row:nth-child(3) > div:nth-child(2) > span.price"
    )
    return price.text
```

**åæœ**
```
1. ç½‘ç«™å¾®å°æ”¹åŠ¨å°±å¤±æ•ˆ
2. ä¸åŒé¡µé¢ç»“æ„æ— æ³•å¤ç”¨
3. ç»´æŠ¤æˆæœ¬é«˜
4. éš¾ä»¥è°ƒè¯•
```

**æ­£ç¡®åšæ³•**
```python
# âœ… æ­£ç¡®: å¤šé‡é€‰æ‹©å™¨ + å®¹é”™
def get_price(html):
    soup = BeautifulSoup(html)

    # æŒ‰ä¼˜å…ˆçº§å°è¯•å¤šä¸ªé€‰æ‹©å™¨
    selectors = [
        '[data-price]::attr(data-price)',  # æ•°æ®å±æ€§ä¼˜å…ˆ
        '.product-price .price',           # è¯­ä¹‰class
        '.price',                          # é€šç”¨class
        '#price',                          # ID
    ]

    for selector in selectors:
        element = soup.select_one(selector)
        if element:
            price_text = element.get('data-price') or element.text
            if price_text:
                return clean_price(price_text)

    # éƒ½å¤±è´¥äº†ï¼Œè®°å½•å¹¶è¿”å›None
    logger.warning(f"æ— æ³•æ‰¾åˆ°ä»·æ ¼å…ƒç´ ")
    return None

def clean_price(text):
    """æ¸…æ´—ä»·æ ¼æ–‡æœ¬"""
    import re
    match = re.search(r'[\d,.]+', text)
    return match.group() if match else None
```

---

### âš ï¸ WARN-04: ä¸ä¿å­˜ä¸­é—´çŠ¶æ€

**é”™è¯¯è¡Œä¸º**
```python
# âš ï¸ è­¦å‘Š: æ‰€æœ‰æ•°æ®åœ¨å†…å­˜ä¸­ï¼Œç¨‹åºå´©æºƒå…¨ä¸¢å¤±
def scrape_large_site(urls):
    all_data = []  # å†…å­˜ä¸­ç§¯ç´¯

    for url in urls:
        data = scrape(url)
        all_data.append(data)
        # å¦‚æœåœ¨ç¬¬ 9999 ä¸ªURLå´©æºƒï¼Œä¹‹å‰çš„æ•°æ®å…¨ä¸¢

    # æœ€åæ‰ä¿å­˜
    save(all_data)
```

**åæœ**
```
1. å´©æºƒä¸¢å¤±æ‰€æœ‰è¿›åº¦
2. å†…å­˜æº¢å‡ºé£é™©
3. æ— æ³•æ–­ç‚¹ç»­çˆ¬
4. é•¿æ—¶é—´æ— è¾“å‡º
```

**æ­£ç¡®åšæ³•**
```python
# âœ… æ­£ç¡®: å¢é‡ä¿å­˜ + æ–­ç‚¹ç»­çˆ¬
import json

class CheckpointScraper:
    def __init__(self, checkpoint_file="checkpoint.json"):
        self.checkpoint_file = checkpoint_file
        self.completed = set()
        self.load_checkpoint()

    def load_checkpoint(self):
        try:
            with open(self.checkpoint_file) as f:
                data = json.load(f)
                self.completed = set(data.get("completed", []))
        except FileNotFoundError:
            pass

    def save_checkpoint(self):
        with open(self.checkpoint_file, 'w') as f:
            json.dump({"completed": list(self.completed)}, f)

    def scrape(self, urls, output_file):
        with open(output_file, 'a') as f:
            for i, url in enumerate(urls):
                if url in self.completed:
                    continue  # è·³è¿‡å·²å®Œæˆ

                try:
                    data = self.fetch(url)
                    # ç«‹å³å†™å…¥æ–‡ä»¶
                    f.write(json.dumps(data) + '\n')
                    f.flush()

                    self.completed.add(url)

                    # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                    if i % 100 == 0:
                        self.save_checkpoint()

                except Exception as e:
                    logger.error(f"å¤±è´¥: {url}, {e}")

        self.save_checkpoint()
```

---

### âš ï¸ WARN-05: å¿½ç•¥ç¼–ç é—®é¢˜

**é”™è¯¯è¡Œä¸º**
```python
# âš ï¸ è­¦å‘Š: ä¸å¤„ç†ç¼–ç 
def get_content(url):
    response = requests.get(url)
    # å¯èƒ½å‡ºç°ä¹±ç 
    return response.text
```

**åæœ**
```
1. ä¸­æ–‡ä¹±ç 
2. ç‰¹æ®Šå­—ç¬¦ä¸¢å¤±
3. è§£æé”™è¯¯
4. æ•°æ®æŸå
```

**æ­£ç¡®åšæ³•**
```python
# âœ… æ­£ç¡®: æ­£ç¡®å¤„ç†ç¼–ç 
def get_content(url):
    response = requests.get(url)

    # æ–¹æ³•1: è®©requestsè‡ªåŠ¨æ£€æµ‹
    response.encoding = response.apparent_encoding

    # æ–¹æ³•2: ä»å“åº”å¤´è·å–
    content_type = response.headers.get('Content-Type', '')
    if 'charset=' in content_type:
        encoding = content_type.split('charset=')[-1]
        response.encoding = encoding

    # æ–¹æ³•3: ä»HTML metaè·å–
    if '<meta charset=' in response.text[:1000]:
        import re
        match = re.search(r'charset=["\']?([^"\'\s>]+)', response.text[:1000])
        if match:
            response.encoding = match.group(1)

    return response.text
```

---

### âš ï¸ WARN-06: ä¸éªŒè¯æ•°æ®å®Œæ•´æ€§

**é”™è¯¯è¡Œä¸º**
```python
# âš ï¸ è­¦å‘Š: ä¸æ£€æŸ¥æ•°æ®æ˜¯å¦å®Œæ•´
def save_product(product):
    # ç›´æ¥ä¿å­˜ï¼Œä¸éªŒè¯
    db.insert(product)
```

**åæœ**
```
1. å­˜å‚¨ä¸å®Œæ•´æ•°æ®
2. åç»­å¤„ç†å‡ºé”™
3. æ•°æ®è´¨é‡å·®
4. éš¾ä»¥å‘ç°é—®é¢˜
```

**æ­£ç¡®åšæ³•**
```python
# âœ… æ­£ç¡®: éªŒè¯åå†ä¿å­˜
from dataclasses import dataclass
from typing import Optional

@dataclass
class Product:
    id: str
    title: str
    price: float
    url: str
    description: Optional[str] = None

    def validate(self) -> tuple[bool, list[str]]:
        errors = []

        if not self.id:
            errors.append("ç¼ºå°‘id")
        if not self.title or len(self.title) < 2:
            errors.append("æ ‡é¢˜æ— æ•ˆ")
        if self.price is None or self.price < 0:
            errors.append("ä»·æ ¼æ— æ•ˆ")
        if not self.url or not self.url.startswith("http"):
            errors.append("URLæ— æ•ˆ")

        return len(errors) == 0, errors

def save_product(data: dict):
    product = Product(**data)
    is_valid, errors = product.validate()

    if not is_valid:
        logger.warning(f"æ•°æ®éªŒè¯å¤±è´¥: {errors}, æ•°æ®: {data}")
        # ä¿å­˜åˆ°é”™è¯¯é˜Ÿåˆ—ï¼Œè€Œä¸æ˜¯ä¸¢å¼ƒ
        save_to_error_queue(data, errors)
        return False

    db.insert(product)
    return True
```

---

## ä¸‰ã€åæ¨¡å¼æ¡ˆä¾‹ (çœŸå®å¤±è´¥æ¡ˆä¾‹)

### CASE-F01: ç­¾åç®—æ³•çŒœæµ‹å¤±è´¥

**é”™è¯¯åœºæ™¯**
```
ä»»åŠ¡: é‡‡é›†æŸç”µå•†ç½‘ç«™æ•°æ®
è§‚å¯Ÿ: URLä¸­æœ‰signå‚æ•°ï¼Œçœ‹èµ·æ¥æ˜¯MD5
é”™è¯¯å†³å®š: çŒœæµ‹ç­¾åç®—æ³•ï¼Œç›²ç›®å°è¯•
ç»“æœ: æµªè´¹3å°æ—¶ï¼Œå…¨éƒ¨å¤±è´¥
```

**é”™è¯¯è¿‡ç¨‹**
```python
# âŒ é”™è¯¯: ç›²ç›®çŒœæµ‹ç­¾åç®—æ³•
def guess_sign(params):
    # çŒœæµ‹1: ç®€å•MD5
    sign1 = md5(str(params))
    if test_request(sign1): return sign1

    # çŒœæµ‹2: æ’åºåMD5
    sign2 = md5(sorted_params(params))
    if test_request(sign2): return sign2

    # çŒœæµ‹3: åŠ ç›MD5
    for salt in ['', 'key', 'secret', 'salt']:
        sign3 = md5(str(params) + salt)
        if test_request(sign3): return sign3

    # æ°¸è¿œçŒœä¸å¯¹...
```

**ä¸ºä»€ä¹ˆå¤±è´¥**
```
1. ç­¾åç®—æ³•åƒå˜ä¸‡åŒ–ï¼ŒçŒœæµ‹æ•ˆç‡æä½
2. å¯èƒ½æœ‰æ—¶é—´æˆ³éªŒè¯ï¼Œè¿‡æœŸå°±å¤±æ•ˆ
3. å¯èƒ½æœ‰è®¾å¤‡æŒ‡çº¹å‚ä¸
4. æµªè´¹æ—¶é—´ä¸”å®¹æ˜“è¢«å°IP
```

**æ­£ç¡®æ–¹æ³•**
```python
# âœ… æ­£ç¡®: é€šè¿‡JSé€†å‘æ‰¾åˆ°ç®—æ³•
def correct_approach():
    """
    æ­£ç¡®æµç¨‹:
    1. æ‰“å¼€ DevTools â†’ Network
    2. æ‰¾åˆ°å¸¦signçš„è¯·æ±‚
    3. è®¾ç½® XHR æ–­ç‚¹
    4. è¿½è¸ªè°ƒç”¨æ ˆæ‰¾åˆ°ç­¾åå‡½æ•°
    5. åˆ†ææˆ–æå–ç­¾åä»£ç 
    6. å¤ç°ç®—æ³•
    """
    # å‚è€ƒ 09-js-reverse.md
    pass
```

---

### CASE-F02: Cookieä¾èµ–åˆ¤æ–­é”™è¯¯

**é”™è¯¯åœºæ™¯**
```
ä»»åŠ¡: é‡‡é›†éœ€è¦ç™»å½•çš„ç½‘ç«™
è§‚å¯Ÿ: ä¸å¸¦cookieè¿”å›ç©ºæ•°æ®
é”™è¯¯å†³å®š: ä»¥ä¸ºåªéœ€è¦session cookie
ç»“æœ: è¯·æ±‚è¢«æ‹¦æˆªï¼Œæ— æ³•è·å–æ•°æ®
```

**é”™è¯¯è¿‡ç¨‹**
```python
# âŒ é”™è¯¯: åªæºå¸¦éƒ¨åˆ†cookie
def fetch_with_login():
    # ç™»å½•è·å–cookie
    login_response = requests.post("/login", data={...})

    # åªä¿å­˜äº†session cookie
    session_id = login_response.cookies.get("session_id")

    # è¯·æ±‚æ•°æ®
    response = requests.get("/api/data", cookies={"session_id": session_id})
    # è¿”å›ç©ºæˆ–è¢«æ‹¦æˆª
```

**ä¸ºä»€ä¹ˆå¤±è´¥**
```
å®é™…éœ€è¦çš„cookie:
- session_id (ç™»å½•æ€)
- _device (è®¾å¤‡æ ‡è¯†) - ç¼ºå¤±ï¼
- _trace (è¿½è¸ªID) - ç¼ºå¤±ï¼
```

**æ­£ç¡®æ–¹æ³•**
```python
# âœ… æ­£ç¡®: ä½¿ç”¨Sessionä¿æŒæ‰€æœ‰cookie
def correct_approach():
    session = requests.Session()

    # è®¿é—®é¦–é¡µï¼Œè·å–è®¾å¤‡cookie
    session.get("https://example.com")

    # ç™»å½•
    session.post("/login", data={...})

    # æ­¤æ—¶sessionè‡ªåŠ¨ä¿æŒæ‰€æœ‰cookie
    response = session.get("/api/data")

    # æ£€æŸ¥è·å–äº†å“ªäº›cookie
    print("æ‰€æœ‰cookie:", dict(session.cookies))
```

---

### CASE-F03: åçˆ¬ç­‰çº§è¯¯åˆ¤

**é”™è¯¯åœºæ™¯**
```
ä»»åŠ¡: é‡‡é›†äº¬ä¸œå•†å“æ•°æ®
é”™è¯¯åˆ¤æ–­: ä»¥ä¸ºåªæ˜¯æ™®é€šCookieéªŒè¯
é”™è¯¯å†³å®š: ä½¿ç”¨ç®€å•requestsè¯·æ±‚
ç»“æœ: å…¨éƒ¨è¿”å›ç©ºæ•°æ®æˆ–è¢«æ‹¦æˆª
```

**é”™è¯¯è¿‡ç¨‹**
```python
# âŒ é”™è¯¯: ä½ä¼°åçˆ¬éš¾åº¦
def naive_jd_scraper():
    # ä»¥ä¸ºåŠ ä¸ªUser-Agentå°±è¡Œ
    headers = {"User-Agent": "Mozilla/5.0..."}
    response = requests.get(
        "https://item.jd.com/12345.html",
        headers=headers
    )
    # è¿”å›çš„æ˜¯ç©ºé¡µé¢æˆ–éªŒè¯é¡µé¢
```

**ä¸ºä»€ä¹ˆå¤±è´¥**
```
äº¬ä¸œåçˆ¬æœºåˆ¶:
1. h5stç­¾åéªŒè¯ (ç®—æ³•å¤æ‚)
2. è®¾å¤‡æŒ‡çº¹æ£€æµ‹
3. è¡Œä¸ºåˆ†æ
4. é£æ§ç³»ç»Ÿ

ç”¨ç®€å•è¯·æ±‚å®Œå…¨æ— æ³•ç»‘è¿‡
```

**æ­£ç¡®æ–¹æ³•**
```python
# âœ… æ­£ç¡®: å…ˆä¾¦æŸ¥å†å†³ç­–
def correct_approach():
    # 1. å…ˆè¿›è¡Œä¾¦æŸ¥
    analysis = brain.smart_investigate("https://item.jd.com")

    # 2. æ ¹æ®ä¾¦æŸ¥ç»“æœå†³ç­–
    print(f"åçˆ¬ç­‰çº§: {analysis.difficulty}")  # EXTREME
    print(f"éœ€è¦: {analysis.requirements}")
    # éœ€è¦: h5stç­¾å, æµè§ˆå™¨ç¯å¢ƒ, è®¾å¤‡æŒ‡çº¹

    # 3. ä½¿ç”¨æ­£ç¡®çš„æ–¹æ¡ˆ
    # å‚è€ƒ Case-08 äº¬ä¸œh5stæ¡ˆä¾‹
```

---

### CASE-F04: å¹¶å‘è¿‡é«˜å¯¼è‡´å°ç¦

**é”™è¯¯åœºæ™¯**
```
ä»»åŠ¡: é‡‡é›†10ä¸‡æ¡æ•°æ®ï¼Œè¿½æ±‚é€Ÿåº¦
é”™è¯¯å†³å®š: å¼€100å¹¶å‘å¿«é€Ÿé‡‡é›†
ç»“æœ: 5åˆ†é’Ÿå†…IPè¢«æ°¸ä¹…å°ç¦
```

**é”™è¯¯è¿‡ç¨‹**
```python
# âŒ é”™è¯¯: æ¿€è¿›çš„å¹¶å‘ç­–ç•¥
import asyncio

async def aggressive_scraper(urls):
    # 100å¹¶å‘ï¼Œæ— å»¶è¿Ÿ
    semaphore = asyncio.Semaphore(100)

    async def fetch(url):
        async with semaphore:
            return await session.get(url)

    # åŒæ—¶å‘èµ·å¤§é‡è¯·æ±‚
    tasks = [fetch(url) for url in urls]
    return await asyncio.gather(*tasks)

# 1ç§’å†…å‘é€ä¸Šç™¾è¯·æ±‚ â†’ ç«‹å³è¢«å°
```

**åæœ**
```
1. IPè¢«æ°¸ä¹…å°ç¦
2. è´¦å·è¢«å°
3. æ‰€æœ‰æ•°æ®ä½œåºŸ
4. éœ€è¦æ›´æ¢IPå’Œè´¦å·
```

**æ­£ç¡®æ–¹æ³•**
```python
# âœ… æ­£ç¡®: ä¿å®ˆçš„å¹¶å‘ç­–ç•¥
async def conservative_scraper(urls):
    # ä½å¹¶å‘
    semaphore = asyncio.Semaphore(5)

    async def fetch(url):
        async with semaphore:
            result = await session.get(url)
            # è¯·æ±‚åå»¶è¿Ÿ
            await asyncio.sleep(random.uniform(1, 3))
            return result

    results = []
    for batch in chunks(urls, 100):
        batch_results = await asyncio.gather(*[fetch(u) for u in batch])
        results.extend(batch_results)

        # æ‰¹æ¬¡é—´ä¼‘æ¯
        await asyncio.sleep(60)

    return results
```

**ç»éªŒæ€»ç»“**
```
å®å¯æ…¢ä¸€ç‚¹ï¼Œä¹Ÿä¸è¦è¢«å°:
- 10ä¸‡æ•°æ®ï¼Œ10å¹¶å‘ï¼Œ3ç§’é—´éš” â‰ˆ 8å°æ—¶
- è¢«å°åé‡æ–°å¼€å§‹å¯èƒ½éœ€è¦æ•°å¤©
```

---

### CASE-F05: ä¸æ£€æµ‹èœœç½æ•°æ®

**é”™è¯¯åœºæ™¯**
```
ä»»åŠ¡: é‡‡é›†å•†å“ä»·æ ¼æ•°æ®
é™·é˜±: ç½‘ç«™è¿”å›äº†å‡æ•°æ®
ç»“æœ: é‡‡é›†äº†10ä¸‡æ¡å‡æ•°æ®ï¼Œå…¨éƒ¨ä½œåºŸ
```

**é”™è¯¯è¿‡ç¨‹**
```python
# âŒ é”™è¯¯: ä¸éªŒè¯æ•°æ®çœŸå®æ€§
def scrape_prices(urls):
    results = []
    for url in urls:
        data = fetch_and_parse(url)
        # ç›´æ¥å­˜å‚¨ï¼Œä¸éªŒè¯
        results.append(data)

    save_all(results)
    # äº‹åå‘ç°ï¼šæ‰€æœ‰ä»·æ ¼éƒ½æ˜¯ 0.01
```

**èœœç½ç‰¹å¾**
```
1. ä»·æ ¼å¼‚å¸¸ (0.01, 9999999)
2. åç§°é‡å¤æˆ–è§„å¾‹æ€§å¼º
3. é“¾æ¥ä¸å¯è®¿é—®
4. æ—¶é—´æˆ³ç›¸åŒ
5. æ•°æ®è¿‡äºè§„æ•´
```

**æ­£ç¡®æ–¹æ³•**
```python
# âœ… æ­£ç¡®: æ•°æ®çœŸå®æ€§éªŒè¯
def scrape_with_validation(urls):
    results = []
    suspicious_count = 0

    for url in urls:
        data = fetch_and_parse(url)

        # èœœç½æ£€æµ‹
        if is_honeypot(data):
            suspicious_count += 1
            logger.warning(f"ç–‘ä¼¼èœœç½æ•°æ®: {data}")

            # è¿ç»­å¤šæ¬¡å¯ç–‘ï¼Œåœæ­¢é‡‡é›†
            if suspicious_count > 10:
                logger.error("æ£€æµ‹åˆ°èœœç½ï¼Œåœæ­¢é‡‡é›†")
                break

            continue

        results.append(data)

    return results

def is_honeypot(data):
    """èœœç½æ£€æµ‹"""
    checks = [
        data.get('price', 0) < 0.1,  # ä»·æ ¼è¿‡ä½
        data.get('price', 0) > 100000,  # ä»·æ ¼è¿‡é«˜
        len(set(data.get('name', '').split())) < 2,  # åç§°å¤ªçŸ­
        'test' in data.get('name', '').lower(),  # æµ‹è¯•æ•°æ®
    ]
    return any(checks)
```

---

## å››ã€å†³ç­–åæ¨¡å¼

### DEC-01: è¿‡æ—©ä¼˜åŒ–

**é”™è¯¯æ¨¡å¼**
```
è¿˜æ²¡è·‘é€šå°±å¼€å§‹ä¼˜åŒ–æ€§èƒ½
è¿˜æ²¡éªŒè¯å°±å¼€å§‹è€ƒè™‘æ‰©å±•æ€§
è¿˜æ²¡æ•°æ®å°±å¼€å§‹è®¾è®¡æ•°æ®åº“
```

**æ­£ç¡®é¡ºåº**
```
1. å…ˆè·‘é€šä¸€ä¸ªURL
2. éªŒè¯æ•°æ®æ­£ç¡®
3. æ‰©å±•åˆ°æ‰¹é‡
4. å†è€ƒè™‘æ€§èƒ½ä¼˜åŒ–
```

---

### DEC-02: è¿‡åº¦è®¾è®¡

**é”™è¯¯æ¨¡å¼**
```
ç®€å•ä»»åŠ¡ç”¨å¤æ‚æ¶æ„
é‡‡é›†100æ¡æ•°æ®å´éƒ¨ç½²åˆ†å¸ƒå¼ç³»ç»Ÿ
ä¸´æ—¶è„šæœ¬å´å†™æˆæ¡†æ¶
```

**æ­£ç¡®åŸåˆ™**
```
KISS - Keep It Simple, Stupid
å¤Ÿç”¨å°±å¥½ï¼ŒæŒ‰éœ€æ‰©å±•
```

---

### DEC-03: å¿½è§†ä¾¦æŸ¥

**é”™è¯¯æ¨¡å¼**
```
æ‹¿åˆ°URLå°±å¼€å§‹å†™ä»£ç 
ä¸åˆ†æç›´æ¥ç”¨ä»¥å‰çš„æ–¹æ¡ˆ
å¤±è´¥äº†æ‰å›å¤´çœ‹
```

**æ­£ç¡®åŸåˆ™**
```
ä¾¦æŸ¥ä¼˜å…ˆ
çŸ¥å·±çŸ¥å½¼
ä¸€åˆ†ä¾¦æŸ¥ï¼Œååˆ†å›æŠ¥
```

---

## äº”ã€æ£€æŸ¥æ¸…å•

### å¼€å§‹å‰æ£€æŸ¥

```markdown
â–¡ æ˜¯å¦è¿›è¡Œäº†å……åˆ†ä¾¦æŸ¥?
â–¡ æ˜¯å¦äº†è§£ç›®æ ‡ç½‘ç«™çš„åçˆ¬ç­‰çº§?
â–¡ æ˜¯å¦æœ‰åˆæ³•çš„è®¿é—®æƒé™?
â–¡ æ˜¯å¦å‡†å¤‡äº†é”™è¯¯å¤„ç†æ–¹æ¡ˆ?
â–¡ æ˜¯å¦è®¾ç½®äº†é‡è¯•ä¸Šé™?
â–¡ æ˜¯å¦æœ‰æ•°æ®éªŒè¯æœºåˆ¶?
```

### ç¼–ç æ—¶æ£€æŸ¥

```markdown
â–¡ æ˜¯å¦å¤„ç†äº†æ‰€æœ‰å¼‚å¸¸?
â–¡ æ˜¯å¦æ£€æŸ¥äº†å“åº”çŠ¶æ€?
â–¡ æ˜¯å¦æœ‰åˆç†çš„å»¶è¿Ÿ?
â–¡ æ˜¯å¦ä¿å­˜äº†ä¸­é—´çŠ¶æ€?
â–¡ æ˜¯å¦è®°å½•äº†æ—¥å¿—?
â–¡ æ˜¯å¦é¿å…äº†ç¡¬ç¼–ç ?
```

### è¿è¡Œæ—¶æ£€æŸ¥

```markdown
â–¡ æ•°æ®æ˜¯å¦çœŸå®æœ‰æ•ˆ?
â–¡ æ˜¯å¦æœ‰å¼‚å¸¸æ¨¡å¼?
â–¡ æˆåŠŸç‡æ˜¯å¦æ­£å¸¸?
â–¡ æ˜¯å¦è¢«é™æµ/å°ç¦?
â–¡ å†…å­˜ä½¿ç”¨æ˜¯å¦æ­£å¸¸?
â–¡ æ˜¯å¦æœ‰è¿›åº¦è¾“å‡º?
```

---

## è¯Šæ–­æ—¥å¿—æ ¼å¼

```yaml
anti_pattern_detected:
  timestamp: "æ£€æµ‹æ—¶é—´"
  pattern_id: "WARN-01"
  pattern_name: "è¯·æ±‚é¢‘ç‡è¿‡é«˜"
  severity: "warning"

  context:
    code_location: "scraper.py:42"
    current_behavior: "æ— å»¶è¿Ÿæ‰¹é‡è¯·æ±‚"
    risk_assessment: "é«˜é£é™©ï¼Œå¯èƒ½å¯¼è‡´IPå°ç¦"

  correction:
    recommended_action: "æ·»åŠ 1-3ç§’éšæœºå»¶è¿Ÿ"
    code_example: "time.sleep(random.uniform(1, 3))"

  outcome:
    action_taken: "å·²ä¿®æ­£"
    result: "è¯·æ±‚æ­£å¸¸ï¼Œæœªè¢«å°ç¦"
```

---

## å…³è”æ¨¡å—

- **17-feedback-loop.md** - ä»é”™è¯¯ä¸­å­¦ä¹ 
- **19-fault-decision-tree.md** - æ•…éšœå¤„ç†
- **08-diagnosis.md** - é—®é¢˜è¯Šæ–­
- **18-brain-controller.md** - å†³ç­–æ§åˆ¶
